{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Boosting for Estimating Nuisance Parameters in GRF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the DGP from the GRF Github ReadMe, I evaluated three methods of predicting E(Y|X) and E(W|X) for input in grf's causal forest: \n",
    "\n",
    "- Using a single random forest (existing implementation) \n",
    "- Using boosted regression trees (common boosting format)\n",
    "- Using boosted random forests with learning rate of 1 (as in Stefan's email) \n",
    "\n",
    "I evaluate each method using the MSE of Y.hat and E(Y|X), of W.hat and E(W|X), and of Tau.hat and TAU from the resulting causal forest, for each of the three methodologies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>method</th><th scope=col>mse.y</th><th scope=col>mse.w</th><th scope=col>mse.tau</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Random Forest  </td><td>0.1530409      </td><td>0.004749936    </td><td>0.02866853     </td></tr>\n",
       "\t<tr><td>Boosted Trees  </td><td>0.2754421      </td><td>0.002266157    </td><td>0.03410535     </td></tr>\n",
       "\t<tr><td>Boosted Forests</td><td>0.1033834      </td><td>0.003153214    </td><td>0.03739853     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " method & mse.y & mse.w & mse.tau\\\\\n",
       "\\hline\n",
       "\t Random Forest   & 0.1530409       & 0.004749936     & 0.02866853     \\\\\n",
       "\t Boosted Trees   & 0.2754421       & 0.002266157     & 0.03410535     \\\\\n",
       "\t Boosted Forests & 0.1033834       & 0.003153214     & 0.03739853     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "method | mse.y | mse.w | mse.tau | \n",
       "|---|---|---|\n",
       "| Random Forest   | 0.1530409       | 0.004749936     | 0.02866853      | \n",
       "| Boosted Trees   | 0.2754421       | 0.002266157     | 0.03410535      | \n",
       "| Boosted Forests | 0.1033834       | 0.003153214     | 0.03739853      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  method          mse.y     mse.w       mse.tau   \n",
       "1 Random Forest   0.1530409 0.004749936 0.02866853\n",
       "2 Boosted Trees   0.2754421 0.002266157 0.03410535\n",
       "3 Boosted Forests 0.1033834 0.003153214 0.03739853"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse.y = c(mse.y.rf, mse.y.boost,mse.y.both)\n",
    "mse.w = c(mse.w.rf,mse.w.boost,mse.w.both)\n",
    "mse.tau = c(mse.tau.rf,mse.tau.boost,mse.tau.both)\n",
    "data.frame(method=c(\"Random Forest\",\"Boosted Trees\", \"Boosted Forests\"),mse.y= mse.y,mse.w=mse.w,mse.tau=mse.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosted forests improve estimation of E(Y|X) significantly. The estimation of E(W|X) improves vs random forests but not vs boosted trees. The MSE of Tau actually increases from random forests to boosted forests. \n",
    "\n",
    "The rest of the notebook contains the code and further details on the results. I also print the results for test_calibration for each of the three methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now I have relied on as simple R implementation of boosted forests with shrinkage of 1 to estimate Y.hat and W.hat. If we were to add this functionality to GRF, I likely would write a C++ implementation using the existing regression_tree function in GRF and user-specified boosting-specific parameters (like learning rate and the number of forests used). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(gbm)\n",
    "library(grf)\n",
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Baseline using regression_forest\n",
    "Modified this DGP from GRF Github Readme.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4000; p = 20\n",
    "X = matrix(rnorm(n * p), n, p)\n",
    "TAU = 1 / (1 + exp(-X[, 3]))\n",
    "E.W = 1 / (1 + exp(-X[, 1] - X[, 2]))\n",
    "W = rbinom(n ,1, E.W)\n",
    "E.Y = pmax(X[, 2] + X[, 3], 0) + rowMeans(X[, 4:6]) / 2 + W * TAU\n",
    "Y = E.Y + rnorm(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.W = regression_forest(X, W, tune.parameters = TRUE)\n",
    "W.hat = predict(forest.W)$predictions\n",
    "\n",
    "forest.Y = regression_forest(X, Y, tune.parameters = TRUE)\n",
    "Y.hat = predict(forest.Y)$predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau.forest = causal_forest(X, Y, W,\n",
    "                           W.hat = W.hat, Y.hat = Y.hat,\n",
    "                           tune.parameters = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Best linear fit using forest predictions (on held-out data)\n",
       "as well as the mean forest prediction as regressors, along\n",
       "with heteroskedasticity-robust (HC3) SEs:\n",
       "\n",
       "                               Estimate Std. Error t value Pr(>|t|)    \n",
       "mean.forest.prediction          1.00449    0.07853 12.7912   <2e-16 ***\n",
       "differential.forest.prediction  0.86987    0.54648  1.5918   0.1115    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_calibration(tau.forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau.hat.forest = predict(tau.forest)$predictions\n",
    "mse.tau.rf = mean((TAU - tau.hat.forest)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Boosted trees don't work as well as the random forest regression alone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>null device:</strong> 1"
      ],
      "text/latex": [
       "\\textbf{null device:} 1"
      ],
      "text/markdown": [
       "**null device:** 1"
      ],
      "text/plain": [
       "null device \n",
       "          1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ERROR",
     "evalue": "Error in plot.window(...): need finite 'ylim' values\n",
     "output_type": "error",
     "traceback": [
      "Error in plot.window(...): need finite 'ylim' values\nTraceback:\n",
      "1. predict(object = boost.W, n.trees = gbm.perf(boost.W), type = \"response\")",
      "2. predict.gbm(object = boost.W, n.trees = gbm.perf(boost.W), type = \"response\")",
      "3. gbm.perf(boost.W)",
      "4. plot(object$train.error, ylim = ylim, type = \"l\", xlab = \"Iteration\", \n .     ylab = ylab)",
      "5. plot.default(object$train.error, ylim = ylim, type = \"l\", xlab = \"Iteration\", \n .     ylab = ylab)",
      "6. localWindow(xlim, ylim, log, asp, ...)",
      "7. plot.window(...)"
     ]
    }
   ],
   "source": [
    "dev.off() \n",
    "boost.Y = gbm(formula = Y~ .,\n",
    "              distribution = \"gaussian\",\n",
    "              data = data.frame(Y=Y,X=X),\n",
    "                 n.trees = 2000,\n",
    "                  shrinkage = 0.1, \n",
    "                  cv.folds = 5,\n",
    "                  n.cores = 1)\n",
    "\n",
    "Y.hat.boost = predict(object=boost.Y,n.trees=gbm.perf(boost.Y),type=\"response\")\n",
    "\n",
    "boost.W = gbm(formula = W~ .,\n",
    "              distribution = \"bernoulli\",\n",
    "              data = data.frame(W=W,X=X),\n",
    "                 n.trees = 2000,\n",
    "                  shrinkage = 0.1, \n",
    "                  cv.folds = 5,\n",
    "                  n.cores = 1)\n",
    "\n",
    "W.hat.boost = predict(object=boost.W,n.trees=gbm.perf(boost.W),type=\"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost.tau.forest = causal_forest(X,Y,W,\n",
    "                                W.hat=W.hat.boost,Y.hat=Y.hat.boost,\n",
    "                                tune.parameters = TRUE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse.y.rf = mean((Y.hat-E.Y)^2)\n",
    "mse.y.boost = mean((Y.hat.boost-E.Y)^2)\n",
    "\n",
    "mse.w.rf = mean((W.hat-E.W)^2)\n",
    "mse.w.boost = mean((W.hat.boost-E.W)^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Best linear fit using forest predictions (on held-out data)\n",
       "as well as the mean forest prediction as regressors, along\n",
       "with heteroskedasticity-robust (HC3) SEs:\n",
       "\n",
       "                               Estimate Std. Error t value Pr(>|t|)    \n",
       "mean.forest.prediction          0.99732    0.09224 10.8123   <2e-16 ***\n",
       "differential.forest.prediction  0.53819    0.47455  1.1341   0.2568    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_calibration(boost.tau.forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau.hat.forest.boost = predict(boost.tau.forest)$predictions\n",
    "mse.tau.boost = mean((TAU - tau.hat.forest.boost)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find that the differential.forest.prediction is lower using boosting of regression trees compared to when the random forests are used to predict Y.hat and W.hat. The prediction of E.Y is worse as well while the prediction of E.W. is a bit better for forests. The prediction of tau is substantially worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosted random forests have mixed results vs random forests alone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted_forest_predictions <- function(Y,X,num_trees) {\n",
    "    \n",
    "    forest_1 = regression_forest(X,Y)\n",
    "    e.hat = forest_1$predictions\n",
    "    e = Y- e.hat\n",
    "    Y.hat = e.hat\n",
    "\n",
    "    for (i in 2:num_trees) {\n",
    "        forest = regression_forest(X,e)\n",
    "        e.hat = forest$predictions\n",
    "        e = e - e.hat \n",
    "        Y.hat = Y.hat+ e.hat \n",
    "    }\n",
    "    return(Y.hat)  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.hat.both = boosted_forest_predictions(Y,X,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.hat.both = boosted_forest_predictions(W,X,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse.y.both = mean((Y.hat.both-E.Y)^2)\n",
    "mse.w.both = mean((W.hat.both - E.W)^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Best linear fit using forest predictions (on held-out data)\n",
       "as well as the mean forest prediction as regressors, along\n",
       "with heteroskedasticity-robust (HC3) SEs:\n",
       "\n",
       "                               Estimate Std. Error t value Pr(>|t|)    \n",
       "mean.forest.prediction          0.99732    0.09224 10.8123   <2e-16 ***\n",
       "differential.forest.prediction  0.53819    0.47455  1.1341   0.2568    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "both.tau.forest = causal_forest(X,Y,W,\n",
    "                                W.hat=W.hat.both,Y.hat=Y.hat.both,\n",
    "                                tune.parameters = TRUE) \n",
    "test_calibration(boost.tau.forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau.hat.forest.both = predict(both.tau.forest)$predictions\n",
    "mse.tau.both = mean((TAU - tau.hat.forest.both)^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.0286685258010365"
      ],
      "text/latex": [
       "0.0286685258010365"
      ],
      "text/markdown": [
       "0.0286685258010365"
      ],
      "text/plain": [
       "[1] 0.02866853"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
